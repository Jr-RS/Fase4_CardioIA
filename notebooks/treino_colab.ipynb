{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2e63c0af",
   "metadata": {},
   "source": [
    "# CardioIA - Pipeline de Treinamento no Colab\n",
    "Este notebook automatiza o download, preparação, treinamento e exportação dos modelos do projeto CardioIA."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb8cf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from IPython.display import HTML, display\n",
    "\n",
    "gpus = tf.config.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    print(f\"GPUs detectadas: {gpus}\")\n",
    "else:\n",
    "    display(HTML(\"<h2 style='color: red;'>ALERTA: GPU não detectada! Vá em Runtime -&gt; Change runtime type -&gt; GPU (T4).</h2>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5fdd07e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install tensorflow pandas python-dotenv kaggle tqdm matplotlib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0267116e",
   "metadata": {},
   "source": [
    "## Configuração da API Kaggle\n",
    "Informe as credenciais via variáveis de ambiente (.env com `KAGGLE_USERNAME` e `KAGGLE_KEY`) ou faça upload do arquivo `kaggle.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df75f5a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from google.colab import files\n",
    "\n",
    "load_dotenv()\n",
    "\n",
    "kaggle_dir = Path.home() / '.kaggle'\n",
    "kaggle_dir.mkdir(exist_ok=True)\n",
    "token_path = kaggle_dir / 'kaggle.json'\n",
    "\n",
    "username = os.getenv('KAGGLE_USERNAME')\n",
    "key = os.getenv('KAGGLE_KEY')\n",
    "\n",
    "if username and key:\n",
    "    with token_path.open('w', encoding='utf-8') as fp:\n",
    "        json.dump({'username': username, 'key': key}, fp)\n",
    "    print('Credenciais carregadas a partir do .env.')\n",
    "else:\n",
    "    if not token_path.exists():\n",
    "        print('Faça upload do arquivo kaggle.json exportado das suas credenciais Kaggle.')\n",
    "        uploaded = files.upload()\n",
    "        if 'kaggle.json' in uploaded:\n",
    "            with token_path.open('wb') as fp:\n",
    "                fp.write(uploaded['kaggle.json'])\n",
    "            print('kaggle.json recebido com sucesso.')\n",
    "        else:\n",
    "            raise RuntimeError('kaggle.json não foi enviado. Tente novamente.')\n",
    "    else:\n",
    "        print('kaggle.json já estava presente. Pulando upload.')\n",
    "\n",
    "os.chmod(token_path, 0o600)\n",
    "print('Kaggle API configurada.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a981284e",
   "metadata": {},
   "source": [
    "## ETL: Download, Preparação e Organização do Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78af3a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "TEMP_DIR = Path('temp_data')\n",
    "TEMP_DIR.mkdir(parents=True, exist_ok=True)\n",
    "zip_path = TEMP_DIR / 'nih-chest-x-ray-14-224x224-resized.zip'\n",
    "\n",
    "if not zip_path.exists():\n",
    "    !kaggle datasets download -d xhlulu/nih-chest-x-ray-14-224x224-resized -p temp_data\n",
    "else:\n",
    "    print('Arquivo zip já existe. Pulando download...')\n",
    "\n",
    "!unzip -q -o temp_data/nih-chest-x-ray-14-224x224-resized.zip -d temp_data\n",
    "print('Extração concluída.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23ee6c59",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "TEMP_DIR = Path('temp_data')\n",
    "DATA_DIR = Path('data')\n",
    "TRAIN_DIR = DATA_DIR / 'train'\n",
    "VAL_DIR = DATA_DIR / 'validation'\n",
    "\n",
    "metadata_path = next(TEMP_DIR.rglob('Data_Entry_2017.csv'))\n",
    "df = pd.read_csv(metadata_path)\n",
    "\n",
    "cardio_df = df[df['Finding Labels'].str.contains('Cardiomegaly')].copy()\n",
    "normal_df = df[df['Finding Labels'] == 'No Finding'].copy()\n",
    "\n",
    "cardio_target = min(1000, len(cardio_df))\n",
    "normal_target = min(1000, len(normal_df))\n",
    "\n",
    "if cardio_target < 1000:\n",
    "    print(f'Aviso: apenas {cardio_target} imagens de Cardiomegalia disponíveis.')\n",
    "if normal_target < 1000:\n",
    "    print(f'Aviso: apenas {normal_target} imagens Normais disponíveis.')\n",
    "\n",
    "cardio_df = cardio_df.sample(n=cardio_target, random_state=42).reset_index(drop=True)\n",
    "normal_df = normal_df.sample(n=normal_target, random_state=42).reset_index(drop=True)\n",
    "\n",
    "def split_dataframe(dataframe, train_frac=0.8):\n",
    "    split_idx = int(len(dataframe) * train_frac)\n",
    "    treino = dataframe.iloc[:split_idx]\n",
    "    valid = dataframe.iloc[split_idx:]\n",
    "    return treino, valid\n",
    "\n",
    "cardio_train, cardio_val = split_dataframe(cardio_df)\n",
    "normal_train, normal_val = split_dataframe(normal_df)\n",
    "\n",
    "dataset_root_candidates = [p for p in TEMP_DIR.iterdir() if p.is_dir()]\n",
    "dataset_root = dataset_root_candidates[0] if dataset_root_candidates else TEMP_DIR\n",
    "\n",
    "images_dir = None\n",
    "for name in ['images_224', 'images']:\n",
    "    candidate = dataset_root / name\n",
    "    if candidate.exists() and candidate.is_dir():\n",
    "        images_dir = candidate\n",
    "        break\n",
    "\n",
    "if images_dir is None:\n",
    "    for candidate in dataset_root.rglob('*'):\n",
    "        if candidate.is_dir() and 'image' in candidate.name.lower():\n",
    "            sample_file = next(candidate.glob('*.png'), None) or next(candidate.glob('*.jpg'), None)\n",
    "            if sample_file:\n",
    "                images_dir = candidate\n",
    "                break\n",
    "\n",
    "if images_dir is None:\n",
    "    raise RuntimeError('Diretório de imagens não encontrado após extração.')\n",
    "\n",
    "print(f'Imagens localizadas em: {images_dir}')\n",
    "\n",
    "for subset_dir in [TRAIN_DIR, VAL_DIR]:\n",
    "    if subset_dir.exists():\n",
    "        shutil.rmtree(subset_dir)\n",
    "    subset_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "def copiar_imagens(dataframe, subset, label):\n",
    "    destino = DATA_DIR / subset / label\n",
    "    destino.mkdir(parents=True, exist_ok=True)\n",
    "    sucesso = 0\n",
    "    for img_name in tqdm(dataframe['Image Index'], desc=f'{subset}/{label}', unit='img'):\n",
    "        origem = images_dir / img_name\n",
    "        if origem.exists():\n",
    "            shutil.copy2(origem, destino / img_name)\n",
    "            sucesso += 1\n",
    "        else:\n",
    "            tqdm.write(f'Arquivo ausente: {img_name}')\n",
    "    return sucesso\n",
    "\n",
    "print('Organizando dataset...')\n",
    "copiar_imagens(cardio_train, 'train', 'cardiomegaly')\n",
    "copiar_imagens(normal_train, 'train', 'normal')\n",
    "copiar_imagens(cardio_val, 'validation', 'cardiomegaly')\n",
    "copiar_imagens(normal_val, 'validation', 'normal')\n",
    "\n",
    "shutil.rmtree(TEMP_DIR, ignore_errors=True)\n",
    "print(f'Dataset pronto: {cardio_target} Cardiomegalia, {normal_target} Normal.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9c84da2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_DIR = Path('data')\n",
    "for subset in ['train', 'validation']:\n",
    "    for label in ['cardiomegaly', 'normal']:\n",
    "        destino = DATA_DIR / subset / label\n",
    "        count = sum(1 for _ in destino.glob('*')) if destino.exists() else 0\n",
    "        print(f'{subset}/{label}: {count} imagens')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c92088be",
   "metadata": {},
   "source": [
    "## Treinamento: ResNet50 (Transfer Learning) vs CNN Simples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f400c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "SRC_DIR = PROJECT_ROOT / 'src'\n",
    "if not SRC_DIR.exists():\n",
    "    raise FileNotFoundError(f'Diretório src não encontrado em {SRC_DIR}. Faça upload do código-fonte do projeto.')\n",
    "\n",
    "if str(SRC_DIR) not in sys.path:\n",
    "    sys.path.append(str(SRC_DIR))\n",
    "\n",
    "import data_preprocessing\n",
    "import model_resnet\n",
    "import model_simple_cnn\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a9cc34a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "DATA_BASE = Path('data')\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "train_gen_resnet, val_gen_resnet = data_preprocessing.configurar_geradores(\n",
    "    diretorio_base=DATA_BASE,\n",
    "    batch_size=32,\n",
    "    target_size=(224, 224),\n",
    ")\n",
    "\n",
    "resnet_model = model_resnet.construir_modelo(learning_rate=1e-4)\n",
    "\n",
    "resnet_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath=str(MODELS_DIR / 'best_model.h5'), monitor='val_loss', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7),\n",
    "]\n",
    "\n",
    "history_resnet = resnet_model.fit(\n",
    "    train_gen_resnet,\n",
    "    epochs=20,\n",
    "    validation_data=val_gen_resnet,\n",
    "    callbacks=resnet_callbacks,\n",
    "    verbose=1,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3589832c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_gen_cnn, val_gen_cnn = data_preprocessing.configurar_geradores(\n",
    "    diretorio_base=DATA_BASE,\n",
    "    batch_size=32,\n",
    "    target_size=(224, 224),\n",
    ")\n",
    "\n",
    "cnn_model = model_simple_cnn.construir_modelo(learning_rate=1e-3)\n",
    "\n",
    "cnn_callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True),\n",
    "    ModelCheckpoint(filepath=str(MODELS_DIR / 'cnn_baseline_best.h5'), monitor='val_loss', save_best_only=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=5, min_lr=1e-7),\n",
    "]\n",
    "\n",
    "history_cnn = cnn_model.fit(\n",
    "    train_gen_cnn,\n",
    "    epochs=20,\n",
    "    validation_data=val_gen_cnn,\n",
    "    callbacks=cnn_callbacks,\n",
    "    verbose=1,\n",
    ")\n",
    "\n",
    "cnn_model.save(MODELS_DIR / 'cnn_baseline_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66d9b7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_history(history, titulo):\n",
    "    metric_map = {'loss': 'Loss', 'accuracy': 'Acurácia'}\n",
    "    for metric, label in metric_map.items():\n",
    "        plt.figure(figsize=(7, 4))\n",
    "        plt.plot(history.history[metric], label='Treino')\n",
    "        plt.plot(history.history[f'val_{metric}'], label='Validação')\n",
    "        plt.title(f'{titulo} - {label}')\n",
    "        plt.xlabel('Épocas')\n",
    "        plt.ylabel(label)\n",
    "        plt.legend()\n",
    "        plt.grid(True, linestyle='--', alpha=0.3)\n",
    "        plt.show()\n",
    "\n",
    "plot_history(history_resnet, 'ResNet50')\n",
    "plot_history(history_cnn, 'CNN Simples')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd60a4e4",
   "metadata": {},
   "source": [
    "## Exportação dos Modelos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c6c5821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "from google.colab import files\n",
    "\n",
    "MODELS_DIR = Path('models')\n",
    "MODELS_DIR.mkdir(exist_ok=True)\n",
    "\n",
    "archive_path = shutil.make_archive('cardioia_models', 'zip', root_dir=MODELS_DIR)\n",
    "print(f'Arquivo gerado: {archive_path}')\n",
    "files.download(archive_path)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
